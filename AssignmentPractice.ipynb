{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "nltk.corpus.gutenberg.fileids()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 1,
       "text": [
        "['austen-emma.txt',\n",
        " 'austen-persuasion.txt',\n",
        " 'austen-sense.txt',\n",
        " 'bible-kjv.txt',\n",
        " 'blake-poems.txt',\n",
        " 'bryant-stories.txt',\n",
        " 'burgess-busterbrown.txt',\n",
        " 'carroll-alice.txt',\n",
        " 'chesterton-ball.txt',\n",
        " 'chesterton-brown.txt',\n",
        " 'chesterton-thursday.txt',\n",
        " 'edgeworth-parents.txt',\n",
        " 'melville-moby_dick.txt',\n",
        " 'milton-paradise.txt',\n",
        " 'shakespeare-caesar.txt',\n",
        " 'shakespeare-hamlet.txt',\n",
        " 'shakespeare-macbeth.txt',\n",
        " 'whitman-leaves.txt']"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "emma = nltk.corpus.gutenberg.words('austen-emma.txt')\n",
      "len(emma)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# another way\n",
      "from nltk.corpus import gutenberg\n",
      "gutenberg.fileids()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "emma = gutenberg.words('austen-emma.txt')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# NLTK webtext contains data from discussion forums etc\n",
      "from nltk.corpus import nps_chat"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The Brown Corpus is a convenient resource for studying systematic differences between genres, a kind of linguistic inquiry known as stylistics\n",
      "A modal verb (also modal, modal auxiliary verb, modal auxiliary) is a type of auxiliary verb that is used to indicate modality \u2013 that is, likelihood, ability, permission, and obligation.[1] Examples include the English verbs can/could, may/might, must, will/would, and shall/should.\n",
      "\n",
      "Tex corpus structures can be isolated, overlapping, categorized and temporal"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import brown\n",
      "cfd = nltk.ConditionalFreqDist((genre, word) for genre in brown.categories() for word in brown.words(categories=genre))\n",
      "genres = ['news', 'religion', 'hobbies', 'science_fiction', 'romance', 'humor']\n",
      "modals = ['can', 'could', 'may', 'might', 'must', 'will']\n",
      "cfd.tabulate(conditions=genres, samples=modals)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#What does conditional frequqnecy do/give?"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Question1\n",
      "names = nltk.corpus.names\n",
      "names.fileids()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cfd = nltk.ConditionalFreqDist((fileid,name[0].lower()) for fileid in names.fileids() for name in names.words(fileid))\n",
      "cfd.plot()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cfd = nltk.ConditionalFreqDist((fileid,name[-1].lower()) for fileid in names.fileids() for name in names.words(fileid))\n",
      "cfd.plot()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import brown\n",
      "brown.categories()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "import string\n",
      "from nltk.book import *\n",
      "fq = FreqDist([w.lower() for w in brown.words() if w not in string.punctuation and w.isalpha()])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "*** Introductory Examples for the NLTK Book ***\n",
        "Loading text1, ..., text9 and sent1, ..., sent9\n",
        "Type the name of the text or sentence to view it.\n",
        "Type: 'texts()' or 'sents()' to list the materials.\n",
        "text1:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Moby Dick by Herman Melville 1851\n",
        "text2:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Sense and Sensibility by Jane Austen 1811\n",
        "text3:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " The Book of Genesis\n",
        "text4:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Inaugural Address Corpus\n",
        "text5:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Chat Corpus\n",
        "text6: Monty Python and the Holy Grail\n",
        "text7:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Wall Street Journal\n",
        "text8: Personals Corpus\n",
        "text9:"
       ]
      },
      {
       "ename": "NameError",
       "evalue": "name 'brown' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-2-75306ad820be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mfq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFreqDist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbrown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunctuation\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misalpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;31mNameError\u001b[0m: name 'brown' is not defined"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " The Man Who Was Thursday by G . K . Chesterton 1908\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fq.keys()[:500]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'fq' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-4-43c2c6c102b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;31mNameError\u001b[0m: name 'fq' is not defined"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fq.plot(50);fq.plot(50,100),fq.plot(100,150);fq.plot(150,200);fq.plot(250,300);fq.plot(300,350)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "[(rank+1,k) for rank,k in enumerate(fq.keys()) if k not in stopwords.words('english') and rank <500]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'fq' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-3-e5f8a946942e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mrank\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;31mNameError\u001b[0m: name 'fq' is not defined"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import stopwords\n",
      "[(rank+1,k) for rank,k in enumerate(fq.keys()) if k in stopwords.words('english')]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for i in stopwords.words('english'):\n",
      "    if i == \"the\":\n",
      "        print i\n",
      "    #print i"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import math\n",
      "math.log(1,2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Question 3\n",
      "from nltk.corpus import stopwords\n",
      "sorted(stopwords.words('english'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Your own corpus\n",
      "from nltk.corpus import PlaintextCorpusReader\n",
      "corpus_root =os.getcwd()+\"/assignment2/Data\"\n",
      "wordlists = PlaintextCorpusReader(corpus_root, '.*')\n",
      "wordlists.fileids()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(wordlists.words())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "myfreq = FreqDist([words.lower() for words in wordlists.words() if words not in string.punctuation and words.isalpha() and words not in  stopwords.words('english') ])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "myfreq.keys()[:50]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "print os.pardir+\"/Data/\"\n",
      "print os.getcwd()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import wordnet as wn\n",
      "wn.synsets('dish')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for synset in wn.synsets('bank'):\n",
      "    print synset\n",
      "    print synset.lemma_names\n",
      "    print synset.definition\n",
      "    print [lemma_names for s in synset.hyponyms() for lemma_names in s.lemma_names ]\n",
      "    print \"=====================\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "help(wn)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "09/23/2013"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import feedparser"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "llog = feedparser.parse(\"http://languagelog.ldc.upenn.edu/nll/?feed=atom\")\n",
      "llog['feed']['title']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "llog.entries"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "path = nltk.data.find('corpora/gutenberg/melville-moby_dick.txt')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "path"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "wordlist = [w for w in nltk.corpus.words.words('en') if w.islower()]\n",
      "[w for w in wordlist if re.search('ed$', w)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wsj = sorted(set(nltk.corpus.treebank.words()))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fd = nltk.FreqDist(vs for word in wsj for vs in re.findall(r'[aeiou]{2,}', word))\n",
      "[int(n) for n in re.findall(r'^[0-9]{4}|[0-9]{2}|[0-9]{2}', '2009-12-31')]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#text = \"champ[+s][+s]##Update I knew that I loved the champ, and useing the diaper genie at a friend's house REALLY reinforced that!\"\n",
      "text = \"Works great, no odor, and uses regular bags.\"\n",
      "m = re.findall('\\[([+-])\\d\\]',text)\n",
      "len(set(m))\n",
      "#a = set(m)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import reuters\n",
      "raw = reuters.raw('test/14826')\n",
      "tokens = nltk.word_tokenize(raw)\n",
      "tokens[:20]\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "reuters.fileids()[:50]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nltk.help.upenn_tagset('RB')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tagged_token = nltk.tag.str2tuple('fly/NN')\n",
      "tagged_token"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nltk.corpus.brown.tagged_words()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import brown\n",
      "cfd = nltk.ConditionalFreqDist((genre, word) for genre in brown.categories() for word in brown.words(categories=genre))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cfd.conditions()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "sent = \"best explanation Ive seen Chapter 7 free Foundations Programming ebook Basically NET memory leak occurs referenced objects rooted garbage collected occurs accidentally hold references intended scope Youll know leaks start getting outofmemoryexceptions memory usage goes youd expect perfmon nice memory counters Understanding NETs memory model best way avoiding Specifically understanding garbage collector works references work refer chapter 7 ebook mindful common pitfalls probably common events object registered event object B object stick object B disappears B holds reference solution unregister events youre course good memory profile let object graphs explore nestingreferencing objects references coming root object responsible redgate ants profile JetBrains dotTrace memprofiler really good choices use textonly windbg sos Id strongly recommend commercialvisual product unless youre real guru believe unmanaged code subject typical memory leaks unamanged code references shared managed garbage collector wrong point\"\n",
      "sent"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "'best explanation Ive seen Chapter 7 free Foundations Programming ebook Basically NET memory leak occurs referenced objects rooted garbage collected occurs accidentally hold references intended scope Youll know leaks start getting outofmemoryexceptions memory usage goes youd expect perfmon nice memory counters Understanding NETs memory model best way avoiding Specifically understanding garbage collector works references work refer chapter 7 ebook mindful common pitfalls probably common events object registered event object B object stick object B disappears B holds reference solution unregister events youre course good memory profile let object graphs explore nestingreferencing objects references coming root object responsible redgate ants profile JetBrains dotTrace memprofiler really good choices use textonly windbg sos Id strongly recommend commercialvisual product unless youre real guru believe unmanaged code subject typical memory leaks unamanged code references shared managed garbage collector wrong point'"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#sent = \"java memory\"\n",
      "text = nltk.word_tokenize(sent)\n",
      "nltk.pos_tag(text)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "[('java', 'NN'), ('memory', 'NN')]"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import operator\n",
      "text = nltk.word_tokenize(sent)\n",
      "tags = nltk.pos_tag(text)\n",
      "print tags\n",
      "sorted(tags, key=lambda student: student[1])\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('best', 'JJS'), ('explanation', 'NN'), ('Ive', 'NNP'), ('seen', 'NN'), ('Chapter', 'NNP'), ('7', 'CD'), ('free', 'JJ'), ('Foundations', 'NNS'), ('Programming', 'NNP'), ('ebook', 'NN'), ('Basically', 'NNP'), ('NET', 'NNP'), ('memory', 'NN'), ('leak', 'NN'), ('occurs', 'NNS'), ('referenced', 'VBD'), ('objects', 'NNS'), ('rooted', 'VBN'), ('garbage', 'NN'), ('collected', 'VBN'), ('occurs', 'NNS'), ('accidentally', 'RB'), ('hold', 'VBP'), ('references', 'NNS'), ('intended', 'VBN'), ('scope', 'NN'), ('Youll', 'NNP'), ('know', 'NN'), ('leaks', 'VBZ'), ('start', 'NN'), ('getting', 'VBG'), ('outofmemoryexceptions', 'NNS'), ('memory', 'NN'), ('usage', 'NN'), ('goes', 'VBZ'), ('youd', 'NN'), ('expect', 'NN'), ('perfmon', 'NN'), ('nice', 'NN'), ('memory', 'NN'), ('counters', 'NNS'), ('Understanding', 'VBG'), ('NETs', 'NNS'), ('memory', 'NN'), ('model', 'NN'), ('best', 'JJS'), ('way', 'NN'), ('avoiding', 'VBG'), ('Specifically', 'RB'), ('understanding', 'VBG'), ('garbage', 'NN'), ('collector', 'NN'), ('works', 'NNS'), ('references', 'NNS'), ('work', 'VBP'), ('refer', 'NN'), ('chapter', 'NN'), ('7', 'CD'), ('ebook', 'NN'), ('mindful', 'JJ'), ('common', 'JJ'), ('pitfalls', 'NNS'), ('probably', 'RB'), ('common', 'JJ'), ('events', 'NNS'), ('object', 'VBP'), ('registered', 'VBN'), ('event', 'NN'), ('object', 'NN'), ('B', 'NNP'), ('object', 'NN'), ('stick', 'NN'), ('object', 'NN'), ('B', 'NNP'), ('disappears', 'NNS'), ('B', 'NNP'), ('holds', 'NNS'), ('reference', 'NN'), ('solution', 'NN'), ('unregister', 'NN'), ('events', 'NNS'), ('youre', 'VBP'), ('course', 'NN'), ('good', 'JJ'), ('memory', 'NN'), ('profile', 'NN'), ('let', 'NN'), ('object', 'NN'), ('graphs', 'NNS'), ('explore', 'VBP'), ('nestingreferencing', 'VBG'), ('objects', 'NNS'), ('references', 'NNS'), ('coming', 'VBG'), ('root', 'NN'), ('object', 'NN'), ('responsible', 'JJ'), ('redgate', 'NN'), ('ants', 'NNS'), ('profile', 'IN'), ('JetBrains', 'NNP'), ('dotTrace', 'NNP'), ('memprofiler', 'NN'), ('really', 'RB'), ('good', 'JJ'), ('choices', 'NNS'), ('use', 'VBP'), ('textonly', 'RB'), ('windbg', 'JJ'), ('sos', 'NNS'), ('Id', 'NNP'), ('strongly', 'RB'), ('recommend', 'VBP'), ('commercialvisual', 'JJ'), ('product', 'NN'), ('unless', 'IN'), ('youre', 'NN'), ('real', 'JJ'), ('guru', 'NN'), ('believe', 'NN'), ('unmanaged', 'VBD'), ('code', 'NN'), ('subject', 'NN'), ('typical', 'JJ'), ('memory', 'NN'), ('leaks', 'NNS'), ('unamanged', 'VBD'), ('code', 'NN'), ('references', 'NNS'), ('shared', 'VBD'), ('managed', 'VBN'), ('garbage', 'NN'), ('collector', 'NN'), ('wrong', 'IN'), ('point', 'NN')]\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 13,
       "text": [
        "[('7', 'CD'),\n",
        " ('7', 'CD'),\n",
        " ('profile', 'IN'),\n",
        " ('unless', 'IN'),\n",
        " ('wrong', 'IN'),\n",
        " ('free', 'JJ'),\n",
        " ('mindful', 'JJ'),\n",
        " ('common', 'JJ'),\n",
        " ('common', 'JJ'),\n",
        " ('good', 'JJ'),\n",
        " ('responsible', 'JJ'),\n",
        " ('good', 'JJ'),\n",
        " ('windbg', 'JJ'),\n",
        " ('commercialvisual', 'JJ'),\n",
        " ('real', 'JJ'),\n",
        " ('typical', 'JJ'),\n",
        " ('best', 'JJS'),\n",
        " ('best', 'JJS'),\n",
        " ('explanation', 'NN'),\n",
        " ('seen', 'NN'),\n",
        " ('ebook', 'NN'),\n",
        " ('memory', 'NN'),\n",
        " ('leak', 'NN'),\n",
        " ('garbage', 'NN'),\n",
        " ('scope', 'NN'),\n",
        " ('know', 'NN'),\n",
        " ('start', 'NN'),\n",
        " ('memory', 'NN'),\n",
        " ('usage', 'NN'),\n",
        " ('youd', 'NN'),\n",
        " ('expect', 'NN'),\n",
        " ('perfmon', 'NN'),\n",
        " ('nice', 'NN'),\n",
        " ('memory', 'NN'),\n",
        " ('memory', 'NN'),\n",
        " ('model', 'NN'),\n",
        " ('way', 'NN'),\n",
        " ('garbage', 'NN'),\n",
        " ('collector', 'NN'),\n",
        " ('refer', 'NN'),\n",
        " ('chapter', 'NN'),\n",
        " ('ebook', 'NN'),\n",
        " ('event', 'NN'),\n",
        " ('object', 'NN'),\n",
        " ('object', 'NN'),\n",
        " ('stick', 'NN'),\n",
        " ('object', 'NN'),\n",
        " ('reference', 'NN'),\n",
        " ('solution', 'NN'),\n",
        " ('unregister', 'NN'),\n",
        " ('course', 'NN'),\n",
        " ('memory', 'NN'),\n",
        " ('profile', 'NN'),\n",
        " ('let', 'NN'),\n",
        " ('object', 'NN'),\n",
        " ('root', 'NN'),\n",
        " ('object', 'NN'),\n",
        " ('redgate', 'NN'),\n",
        " ('memprofiler', 'NN'),\n",
        " ('product', 'NN'),\n",
        " ('youre', 'NN'),\n",
        " ('guru', 'NN'),\n",
        " ('believe', 'NN'),\n",
        " ('code', 'NN'),\n",
        " ('subject', 'NN'),\n",
        " ('memory', 'NN'),\n",
        " ('code', 'NN'),\n",
        " ('garbage', 'NN'),\n",
        " ('collector', 'NN'),\n",
        " ('point', 'NN'),\n",
        " ('Ive', 'NNP'),\n",
        " ('Chapter', 'NNP'),\n",
        " ('Programming', 'NNP'),\n",
        " ('Basically', 'NNP'),\n",
        " ('NET', 'NNP'),\n",
        " ('Youll', 'NNP'),\n",
        " ('B', 'NNP'),\n",
        " ('B', 'NNP'),\n",
        " ('B', 'NNP'),\n",
        " ('JetBrains', 'NNP'),\n",
        " ('dotTrace', 'NNP'),\n",
        " ('Id', 'NNP'),\n",
        " ('Foundations', 'NNS'),\n",
        " ('occurs', 'NNS'),\n",
        " ('objects', 'NNS'),\n",
        " ('occurs', 'NNS'),\n",
        " ('references', 'NNS'),\n",
        " ('outofmemoryexceptions', 'NNS'),\n",
        " ('counters', 'NNS'),\n",
        " ('NETs', 'NNS'),\n",
        " ('works', 'NNS'),\n",
        " ('references', 'NNS'),\n",
        " ('pitfalls', 'NNS'),\n",
        " ('events', 'NNS'),\n",
        " ('disappears', 'NNS'),\n",
        " ('holds', 'NNS'),\n",
        " ('events', 'NNS'),\n",
        " ('graphs', 'NNS'),\n",
        " ('objects', 'NNS'),\n",
        " ('references', 'NNS'),\n",
        " ('ants', 'NNS'),\n",
        " ('choices', 'NNS'),\n",
        " ('sos', 'NNS'),\n",
        " ('leaks', 'NNS'),\n",
        " ('references', 'NNS'),\n",
        " ('accidentally', 'RB'),\n",
        " ('Specifically', 'RB'),\n",
        " ('probably', 'RB'),\n",
        " ('really', 'RB'),\n",
        " ('textonly', 'RB'),\n",
        " ('strongly', 'RB'),\n",
        " ('referenced', 'VBD'),\n",
        " ('unmanaged', 'VBD'),\n",
        " ('unamanged', 'VBD'),\n",
        " ('shared', 'VBD'),\n",
        " ('getting', 'VBG'),\n",
        " ('Understanding', 'VBG'),\n",
        " ('avoiding', 'VBG'),\n",
        " ('understanding', 'VBG'),\n",
        " ('nestingreferencing', 'VBG'),\n",
        " ('coming', 'VBG'),\n",
        " ('rooted', 'VBN'),\n",
        " ('collected', 'VBN'),\n",
        " ('intended', 'VBN'),\n",
        " ('registered', 'VBN'),\n",
        " ('managed', 'VBN'),\n",
        " ('hold', 'VBP'),\n",
        " ('work', 'VBP'),\n",
        " ('object', 'VBP'),\n",
        " ('youre', 'VBP'),\n",
        " ('explore', 'VBP'),\n",
        " ('use', 'VBP'),\n",
        " ('recommend', 'VBP'),\n",
        " ('leaks', 'VBZ'),\n",
        " ('goes', 'VBZ')]"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "bigrams =  {'transactional memory': 5, 'memory leak': 24, 'variable memory': 3, 'memory lowlevel': 2, 'memory address': 10, 'virtual memory': 5, 'memory reference': 4, 'page memory': 3, 'memory layout': 14, 'memory condition': 2, 'think memory': 2, 'application memory': 2, 'writing memory': 2, 'memory store': 4, 'block memory': 3, 'memory process': 4, 'memory word': 2, 'cache memory': 3, 'memory available': 2, 'memory managed': 2, 'memory location': 3, 'memory overhead': 4, 'freeing memory': 5, 'memory region': 3, 'memory use': 4, 'allocated memory': 2, 'shared memory': 10, 'related memory': 2, 'memory size': 4, 'memory efficient': 4, 'memory cpu': 2, '3 memory': 2, 'memory using': 4, 'extra memory': 3, 'memory allocation': 7, 'process memory': 7, 'point memory': 3, 'cached memory': 2, 'nets memory': 2, 'memory pressure': 2, 'memory model': 3, 'object memory': 9, 'loaded memory': 3, 'row memory': 3, 'creates memory': 2, 'like memory': 4, 'address memory': 4, 'data memory': 3, 'lot memory': 5, 'memory freed': 4, 'actual memory': 4, 'allocate memory': 3, 'memory runtime': 2, 'memory want': 3, 'memory mapped': 3, 'memory make': 4, 'memory privilege': 2, 'reduce memory': 2, 'memory youll': 2, 'memory create': 4, 'stick memory': 2, 'memory problem': 2, 'memory profiler': 7, 'memory issue': 3, 'memory pool': 2, 'memory space': 2, 'code memory': 3, 'time memory': 3, 'memory management': 12, 'memory profiling': 4, 'reading memory': 3, 'memory just': 3, 'written memory': 4, 'memory access': 6, 'leak memory': 3, 'file memory': 3, 'memory need': 5, 'memory footprint': 4, 'memory block': 4, 'use memory': 5, 'memory allocator': 3, 'memory time': 4, 'string memory': 3, 'net memory': 8, 'memory usage': 14, 'table memory': 2, 'protect memory': 2, 'memory used': 4, 'memory card': 2, 'java memory': 3, \n",
      "            'code memory layout': 2, 'lock page memory': 2, 'net memory profiler': 4, 'written memory tested': 2, 'allocation memory layout': 2, 'memory reference ni': 3}\n",
      "bigrams"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "{'3 memory': 2,\n",
        " 'actual memory': 4,\n",
        " 'address memory': 4,\n",
        " 'allocate memory': 3,\n",
        " 'allocated memory': 2,\n",
        " 'allocation memory layout': 2,\n",
        " 'application memory': 2,\n",
        " 'block memory': 3,\n",
        " 'cache memory': 3,\n",
        " 'cached memory': 2,\n",
        " 'code memory': 3,\n",
        " 'code memory layout': 2,\n",
        " 'creates memory': 2,\n",
        " 'data memory': 3,\n",
        " 'extra memory': 3,\n",
        " 'file memory': 3,\n",
        " 'freeing memory': 5,\n",
        " 'java memory': 3,\n",
        " 'leak memory': 3,\n",
        " 'like memory': 4,\n",
        " 'loaded memory': 3,\n",
        " 'lock page memory': 2,\n",
        " 'lot memory': 5,\n",
        " 'memory access': 6,\n",
        " 'memory address': 10,\n",
        " 'memory allocation': 7,\n",
        " 'memory allocator': 3,\n",
        " 'memory available': 2,\n",
        " 'memory block': 4,\n",
        " 'memory card': 2,\n",
        " 'memory condition': 2,\n",
        " 'memory cpu': 2,\n",
        " 'memory create': 4,\n",
        " 'memory efficient': 4,\n",
        " 'memory footprint': 4,\n",
        " 'memory freed': 4,\n",
        " 'memory issue': 3,\n",
        " 'memory just': 3,\n",
        " 'memory layout': 14,\n",
        " 'memory leak': 24,\n",
        " 'memory location': 3,\n",
        " 'memory lowlevel': 2,\n",
        " 'memory make': 4,\n",
        " 'memory managed': 2,\n",
        " 'memory management': 12,\n",
        " 'memory mapped': 3,\n",
        " 'memory model': 3,\n",
        " 'memory need': 5,\n",
        " 'memory overhead': 4,\n",
        " 'memory pool': 2,\n",
        " 'memory pressure': 2,\n",
        " 'memory privilege': 2,\n",
        " 'memory problem': 2,\n",
        " 'memory process': 4,\n",
        " 'memory profiler': 7,\n",
        " 'memory profiling': 4,\n",
        " 'memory reference': 4,\n",
        " 'memory reference ni': 3,\n",
        " 'memory region': 3,\n",
        " 'memory runtime': 2,\n",
        " 'memory size': 4,\n",
        " 'memory space': 2,\n",
        " 'memory store': 4,\n",
        " 'memory time': 4,\n",
        " 'memory usage': 14,\n",
        " 'memory use': 4,\n",
        " 'memory used': 4,\n",
        " 'memory using': 4,\n",
        " 'memory want': 3,\n",
        " 'memory word': 2,\n",
        " 'memory youll': 2,\n",
        " 'net memory': 8,\n",
        " 'net memory profiler': 4,\n",
        " 'nets memory': 2,\n",
        " 'object memory': 9,\n",
        " 'page memory': 3,\n",
        " 'point memory': 3,\n",
        " 'process memory': 7,\n",
        " 'protect memory': 2,\n",
        " 'reading memory': 3,\n",
        " 'reduce memory': 2,\n",
        " 'related memory': 2,\n",
        " 'row memory': 3,\n",
        " 'shared memory': 10,\n",
        " 'stick memory': 2,\n",
        " 'string memory': 3,\n",
        " 'table memory': 2,\n",
        " 'think memory': 2,\n",
        " 'time memory': 3,\n",
        " 'transactional memory': 5,\n",
        " 'use memory': 5,\n",
        " 'variable memory': 3,\n",
        " 'virtual memory': 5,\n",
        " 'writing memory': 2,\n",
        " 'written memory': 4,\n",
        " 'written memory tested': 2}"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "new_data =  [(u'database handling', 2), (u'data handling pas', 1), \n",
      "             (u'centralize database handling', 1), (u'handling data conversion', 1), \n",
      "             (u'handling databound', 1), (u'handling data', 1), (u'null handling data', 1), (u'metal data handling', 1), \n",
      "             (u'handling binary data', 1), (u'database handling code', 1), (u'data handling', 1), (u'handling databound', 1), \n",
      "             (u'handling database communication', 1), (u'handling database', 1), (u'data exception handlinginstance', 1), (u'having handling database', 1), (u'handling databound control', 1), (u'database handling object', 1), (u'creating database handling', 1), (u'handling large data', 1)]\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for i in new_data:\n",
      "    text = nltk.word_tokenize(i[0])\n",
      "    print nltk.pos_tag(text)\n",
      "\n",
      "import nltk\n",
      "sent1 = \"who is Alfred Einstein and what does he do, does he live in US\"\n",
      "text1 = nltk.pos_tag(sent1.split())\n",
      "out = nltk.ne_chunk(text1)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(u'database', 'NN'), (u'handling', 'NN')]\n",
        "[(u'data', 'NNS'), (u'handling', 'NN'), (u'pas', 'NNS')]\n",
        "[(u'centralize', 'NN'), (u'database', 'NN'), (u'handling', 'NN')]\n",
        "[(u'handling', 'NN'), (u'data', 'NNS'), (u'conversion', 'NN')]\n",
        "[(u'handling', 'NN'), (u'databound', 'VBD')]\n",
        "[(u'handling', 'NN'), (u'data', 'NNS')]\n",
        "[(u'null', 'NN'), (u'handling', 'NN'), (u'data', 'NNS')]\n",
        "[(u'metal', 'NN'), (u'data', 'NNS'), (u'handling', 'NN')]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[(u'handling', 'NN'), (u'binary', 'NN'), (u'data', 'NNS')]\n",
        "[(u'database', 'NN'), (u'handling', 'NN'), (u'code', 'NN')]\n",
        "[(u'data', 'NNS'), (u'handling', 'NN')]\n",
        "[(u'handling', 'NN'), (u'databound', 'VBD')]\n",
        "[(u'handling', 'NN'), (u'database', 'NN'), (u'communication', 'NN')]\n",
        "[(u'handling', 'NN'), (u'database', 'NN')]\n",
        "[(u'data', 'NNS'), (u'exception', 'NN'), (u'handlinginstance', 'NN')]\n",
        "[(u'having', 'VBG'), (u'handling', 'NN'), (u'database', 'NN')]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[(u'handling', 'NN'), (u'databound', 'VBD'), (u'control', 'NN')]\n",
        "[(u'database', 'NN'), (u'handling', 'NN'), (u'object', 'NN')]\n",
        "[(u'creating', 'VBG'), (u'database', 'NN'), (u'handling', 'NN')]\n",
        "[(u'handling', 'NN'), (u'large', 'JJ'), (u'data', 'NNS')]\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nltk.help.upenn_tagset('VBP')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "VBP: verb, present tense, not 3rd person singular\n",
        "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
        "    appear tend stray glisten obtain comprise detest tease attract\n",
        "    emphasize mold postpone sever return wag ...\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wsj = nltk.corpus.treebank.tagged_words(simplify_tags=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wsj[:10]\n",
      "fq = nltk.FreqDist([w[1] for w in wsj])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fq.plot(2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sent = nltk.corpus.treebank.tagged_sents(simplify_tags=True)\n",
      "sent[8]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sent = nltk.corpus.treebank.tagged_sents()\n",
      "sent[8]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "bigrams = nltk.ibigrams(nltk.corpus.treebank.tagged_words())\n",
      "bigrams[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import brown\n",
      "import string\n",
      "brown_lrnd_tagged = brown.tagged_words(simplify_tags=True)\n",
      "tags = [b[1] for (a, b) in nltk.ibigrams(brown_lrnd_tagged) if a[0] == 'have' and b[1] not in string.punctuation]\n",
      "fd = nltk.FreqDist(tags)\n",
      "fd.tabulate()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "from nltk.corpus import brown\n",
      "def process(sentence):\n",
      "    for (w1,t1), (w2,t2), (w3,t3) in nltk.ngrams(sentence,3):\n",
      "        if (t1.startswith('N') and w2 == 'THAT' and t3.startswith('V') ):\n",
      "            print w1, w2, w3"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for tagged_sent in brown.tagged_sents():\n",
      "    process(tagged_sent)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for tagged_sent in brown.tagged_sents():\n",
      "    print tagged_sent\n",
      "    for (w1,t1), (w2,t2), (w3,t3), (w4,t4), (w5,t5) in nltk.ngrams(tagged_sent,5):\n",
      "        if (t1.startswith('V') and w2 == 'THAT' and t3.startswith('V') and t4.startswith('N') and t5.startswith('N')):\n",
      "            print w1, w2, w3, w4, w5"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import brown\n",
      "import math\n",
      "brown_tagged_sents = brown.tagged_sents(categories='news',simplify_tags=True)\n",
      "split = int(math.floor(len(brown_tagged_sents) * 0.9))\n",
      "print split\n",
      "train = brown_tagged_sents[split:]\n",
      "test =  brown_tagged_sents[:split]\n",
      "unigram_tagger = nltk.UnigramTagger(train)\n",
      "unigram_tagger.evaluate(test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import brown\n",
      "brown_tagged_sents = brown.tagged_sents(categories='news')\n",
      "size = int(len(brown_tagged_sents) * 0.9)\n",
      "train_sents = brown_tagged_sents[:size]\n",
      "test_sents = brown_tagged_sents[size:]\n",
      "t0 = nltk.DefaultTagger('NN')\n",
      "t1 = nltk.UnigramTagger(train_sents, backoff=t0)\n",
      "t2 = nltk.BigramTagger(train_sents, backoff=t1)\n",
      "test_tags = [tag for sent in brown.sents(categories='editorial')\n",
      "                  for (word, tag) in t2.tag(sent)]\n",
      "gold_tags = [tag for (word, tag) in brown.tagged_words(categories='editorial')]\n",
      "print nltk.ConfusionMatrix(gold_tags, test_tags)   "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fd = nltk.FreqDist(brown.words(categories='news'))\n",
      "cfd = nltk.ConditionalFreqDist(brown.tagged_words(categories='news'))\n",
      "most_freq_words = fd.keys()[:100]\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "likely_tags = dict((word, cfd[word].max()) for word in most_freq_words)\n",
      "baseline_tagger = nltk.UnigramTagger(model=likely_tags)\n",
      "baseline_tagger.evaluate(brown_tagged_sents)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nltk.help.brown_tagset('BEG')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "nltk.help.upenn_tagset('NNS')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "NNS: noun, common, plural\n",
        "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
        "    divestitures storehouses designs clubs fragrances averages\n",
        "    subjectivists apprehensions muses factory-jobs ...\n"
       ]
      }
     ],
     "prompt_number": 120
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "10/05/2013"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import names\n",
      "import random\n",
      "names = ([(name, 'male') for name in names.words('male.txt')] +\n",
      "         [(name, 'female') for name in names.words('female.txt')])\n",
      "#import random\n",
      "#random.shuffle(names)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Combining elemnst of 2 lists using +\n",
      "t = [(1,2),(3,4)]\n",
      "t1 =[(11,12),(13,14)]\n",
      "t+t1\n",
      "t\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "def gender_features(word):\n",
      "    return {'last_letter': word[-1],\n",
      "            'length': len(word)}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "featuresets = [(gender_features(n), g) for (n,g) in names]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "featuresets"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_set, test_set = featuresets[500:], featuresets[:500]\n",
      "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
      "featuresets_neo = [(gender_features(n), g) for (n,g) in names] + [(length_feature(n), g) for (n,g) in names]\n",
      "print classifier.classify(gender_features('Neo'))\n",
      "print classifier.classify(gender_features('Trinity'))\n",
      "print nltk.classify.accuracy(classifier, test_set)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      " classifier.show_most_informative_features()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import movie_reviews\n",
      "import random\n",
      "documents = [(list(movie_reviews.words(fileid)), category)\n",
      "              for category in movie_reviews.categories()\n",
      "              for fileid in movie_reviews.fileids(category)]\n",
      "random.shuffle(documents)\n",
      "\n",
      "documents[:1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "document= [(list(movie_reviews.words(fileid)),category)\n",
      "            for category in movie_reviews.categories()\n",
      "            for fileid in movie_reviews.fileids(category)]\n",
      "random.shuffle(documents)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Frequency distributions are generally constructed by running a number of experiments, and incrementing the count \n",
      "for a sample every time it is an outcome of an experiment. For example, the following code will produce a \n",
      "frequency distribution that encodes how often each word occurs in a text:\n",
      ">>> fdist = FreqDist()\n",
      ">>> for word in tokenize.whitespace(sent):\n",
      "...    fdist.inc(word.lower())\n",
      "\n",
      "An equivalent way to do this is with the initializer:\n",
      "\n",
      ">>> fdist = FreqDist(word.lower() for word in tokenize.whitespace(sent))"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def pos_features(sentence, i):\n",
      "    features = {\"suffix(1)\": sentence[i][-1:],\n",
      "                \"suffix(2)\": sentence[i][-2:],\n",
      "                \"suffix(3)\": sentence[i][-3:]}\n",
      "    if i == 0:\n",
      "        features[\"prev-word\"] = \"<START>\"\n",
      "    else:\n",
      "        features[\"prev-word\"] = sentence[i-1]\n",
      "    return features\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pos_features(brown.sents()[0], 8)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "brown.sents()[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "10/07/2013"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "text = nltk.word_tokenize(\"The SD500 has only been in my possession for a little over 3 months\")##there was no compro\")\n",
      "tagslist = nltk.pos_tag(text)\n",
      "for t in tagslist:\n",
      "    print t\n",
      "    \n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "nltk.help.upenn_tagset()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "$: dollar\n",
        "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
        "'': closing quotation mark\n",
        "    ' ''\n",
        "(: opening parenthesis\n",
        "    ( [ {\n",
        "): closing parenthesis\n",
        "    ) ] }\n",
        ",: comma\n",
        "    ,\n",
        "--: dash\n",
        "    --\n",
        ".: sentence terminator\n",
        "    . ! ?\n",
        ":: colon or ellipsis\n",
        "    : ; ...\n",
        "CC: conjunction, coordinating\n",
        "    & 'n and both but either et for less minus neither nor or plus so\n",
        "    therefore times v. versus vs. whether yet\n",
        "CD: numeral, cardinal\n",
        "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
        "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
        "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
        "DT: determiner\n",
        "    all an another any both del each either every half la many much nary\n",
        "    neither no some such that the them these this those\n",
        "EX: existential there\n",
        "    there\n",
        "FW: foreign word\n",
        "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
        "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
        "    terram fiche oui corporis ...\n",
        "IN: preposition or conjunction, subordinating\n",
        "    astride among uppon whether out inside pro despite on by throughout\n",
        "    below within for towards near behind atop around if like until below\n",
        "    next into if beside ...\n",
        "JJ: adjective or numeral, ordinal\n",
        "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
        "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
        "    multilingual multi-disciplinary ...\n",
        "JJR: adjective, comparative\n",
        "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
        "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
        "    cozier creamier crunchier cuter ...\n",
        "JJS: adjective, superlative\n",
        "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
        "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
        "    dearest deepest densest dinkiest ...\n",
        "LS: list item marker\n",
        "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
        "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
        "    two\n",
        "MD: modal auxiliary\n",
        "    can cannot could couldn't dare may might must need ought shall should\n",
        "    shouldn't will would\n",
        "NN: noun, common, singular or mass\n",
        "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
        "    investment slide humour falloff slick wind hyena override subhumanity\n",
        "    machinist ...\n",
        "NNP: noun, proper, singular\n",
        "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
        "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
        "    Shannon A.K.C. Meltex Liverpool ...\n",
        "NNPS: noun, proper, plural\n",
        "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
        "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
        "    Apache Apaches Apocrypha ...\n",
        "NNS: noun, common, plural\n",
        "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
        "    divestitures storehouses designs clubs fragrances averages\n",
        "    subjectivists apprehensions muses factory-jobs ...\n",
        "PDT: pre-determiner\n",
        "    all both half many quite such sure this\n",
        "POS: genitive marker\n",
        "    ' 's\n",
        "PRP: pronoun, personal\n",
        "    hers herself him himself hisself it itself me myself one oneself ours\n",
        "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
        "PRP$: pronoun, possessive\n",
        "    her his mine my our ours their thy your\n",
        "RB: adverb\n",
        "    occasionally unabatingly maddeningly adventurously professedly\n",
        "    stirringly prominently technologically magisterially predominately\n",
        "    swiftly fiscally pitilessly ...\n",
        "RBR: adverb, comparative\n",
        "    further gloomier grander graver greater grimmer harder harsher\n",
        "    healthier heavier higher however larger later leaner lengthier less-\n",
        "    perfectly lesser lonelier longer louder lower more ...\n",
        "RBS: adverb, superlative\n",
        "    best biggest bluntest earliest farthest first furthest hardest\n",
        "    heartiest highest largest least less most nearest second tightest worst\n",
        "RP: particle\n",
        "    aboard about across along apart around aside at away back before behind\n",
        "    by crop down ever fast for forth from go high i.e. in into just later\n",
        "    low more off on open out over per pie raising start teeth that through\n",
        "    under unto up up-pp upon whole with you\n",
        "SYM: symbol\n",
        "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
        "TO: \"to\" as preposition or infinitive marker\n",
        "    to\n",
        "UH: interjection\n",
        "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
        "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
        "    man baby diddle hush sonuvabitch ...\n",
        "VB: verb, base form\n",
        "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
        "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
        "    boost brace break bring broil brush build ...\n",
        "VBD: verb, past tense\n",
        "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
        "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
        "    speculated wore appreciated contemplated ...\n",
        "VBG: verb, present participle or gerund\n",
        "    telegraphing stirring focusing angering judging stalling lactating\n",
        "    hankerin' alleging veering capping approaching traveling besieging\n",
        "    encrypting interrupting erasing wincing ...\n",
        "VBN: verb, past participle\n",
        "    multihulled dilapidated aerosolized chaired languished panelized used\n",
        "    experimented flourished imitated reunifed factored condensed sheared\n",
        "    unsettled primed dubbed desired ...\n",
        "VBP: verb, present tense, not 3rd person singular\n",
        "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
        "    appear tend stray glisten obtain comprise detest tease attract\n",
        "    emphasize mold postpone sever return wag ...\n",
        "VBZ: verb, present tense, 3rd person singular\n",
        "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
        "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
        "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
        "WDT: WH-determiner\n",
        "    that what whatever which whichever\n",
        "WP: WH-pronoun\n",
        "    that what whatever whatsoever which who whom whosoever\n",
        "WP$: WH-pronoun, possessive\n",
        "    whose\n",
        "WRB: Wh-adverb\n",
        "    how however whence whenever where whereby whereever wherein whereof why\n",
        "``: opening quotation mark\n",
        "    ` ``\n"
       ]
      }
     ],
     "prompt_number": 122
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "10/30"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk, re, pprint\n",
      "grammar = r\"\"\"\n",
      "  NP: {<DT>?.*<VBG>+.*<NN>}  # chunk determiner/possessive, adjectives and nouns             # chunk sequences of proper nouns\n",
      "      {<NN>?.*<VBG>+.*<NN>}\n",
      "      {<IN>.*<VBG>.*<IN|DT>?.*}\n",
      "      {<VBZ><VBG><DT><NN>}\n",
      "      {<VBZ><IN><VBG>.*<NN>}\n",
      "      {<VBP><IN><VBG><PRP/$><NN>}\n",
      "      {<NN|NNP>+.*<VBG>+.*<NN>}\n",
      "\"\"\"\n",
      "cp = nltk.RegexpParser(grammar)\n",
      "\n",
      "def parser(document):\n",
      "    sentences = nltk.sent_tokenize(document)\n",
      "    for sent in sentences:\n",
      "        print sent\n",
      "        nltk.word_tokenize(sent)\n",
      "        print cp.parse(nltk.pos_tag(nltk.word_tokenize(sent)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 97
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s  = parser(\"She came at the receiving end.\"+ \n",
      "            \" She was the Assistant managing editor.\"+\n",
      "            \" She avoided him by walking on the opposite side of the road.\"+\n",
      "            \" My father decided against \"+\n",
      "            \"postponing his trip to Hungary.\"+\n",
      "              \"Can you sneeze without opening your mouth?\"+\n",
      "              \"I look forward to hearing from you soon.\"+\n",
      "              \" She always puts off going to the dentist.\"+\n",
      "              \" She avoided him by walking on the opposite side of the road.\"+\n",
      "              \" The hardest thing about learning English is understanding the gerund.\")\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "She came at the receiving end.\n",
        "(S She/PRP came/VBD at/IN (NP the/DT receiving/VBG end/NN) ./.)\n",
        "She was the Assistant managing editor.\n",
        "(S\n",
        "  She/PRP\n",
        "  was/VBD\n",
        "  the/DT\n",
        "  Assistant/NNP\n",
        "  (NP managing/VBG editor/NN)\n",
        "  ./.)\n",
        "She avoided him by walking on the opposite side of the road.\n",
        "(S\n",
        "  She/PRP\n",
        "  avoided/VBD\n",
        "  him/PRP\n",
        "  (NP by/IN walking/VBG on/IN)\n",
        "  the/DT\n",
        "  opposite/JJ\n",
        "  side/NN\n",
        "  of/IN\n",
        "  the/DT\n",
        "  road/NN\n",
        "  ./.)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "My father decided against postponing his trip to Hungary.Can you sneeze without opening your mouth?I look forward to hearing from you soon.\n",
        "(S\n",
        "  My/PRP$\n",
        "  father/NN\n",
        "  decided/VBD\n",
        "  against/IN\n",
        "  postponing/NN\n",
        "  his/PRP$\n",
        "  trip/NN\n",
        "  to/TO\n",
        "  Hungary.Can/JJ\n",
        "  you/PRP\n",
        "  sneeze/VBP\n",
        "  (NP without/IN opening/VBG)\n",
        "  your/PRP$\n",
        "  mouth/NN\n",
        "  ?/.\n",
        "  I/PRP\n",
        "  look/VBP\n",
        "  forward/RB\n",
        "  to/TO\n",
        "  hearing/NN\n",
        "  from/IN\n",
        "  you/PRP\n",
        "  soon/RB\n",
        "  ./.)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "She always puts off going to the dentist.\n",
        "(S\n",
        "  She/PRP\n",
        "  always/RB\n",
        "  puts/VBZ\n",
        "  (NP off/IN going/VBG)\n",
        "  to/TO\n",
        "  the/DT\n",
        "  dentist/NN\n",
        "  ./.)\n",
        "She avoided him by walking on the opposite side of the road.\n",
        "(S\n",
        "  She/PRP\n",
        "  avoided/VBD\n",
        "  him/PRP\n",
        "  (NP by/IN walking/VBG on/IN)\n",
        "  the/DT\n",
        "  opposite/JJ\n",
        "  side/NN\n",
        "  of/IN\n",
        "  the/DT\n",
        "  road/NN\n",
        "  ./.)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "The hardest thing about learning English is understanding the gerund.\n",
        "(S\n",
        "  The/DT\n",
        "  hardest/JJS\n",
        "  thing/NN\n",
        "  (NP about/IN learning/VBG)\n",
        "  English/JJ\n",
        "  (NP is/VBZ understanding/VBG the/DT gerund/NN)\n",
        "  ./.)\n"
       ]
      }
     ],
     "prompt_number": 98
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "cp = nltk.RegexpParser('CHUNK: {<V.*> <TO> <V.*>}')\n",
      "brown = nltk.corpus.brown\n",
      "for sent in brown.tagged_sents():\n",
      "    tree = cp.parse(sent)\n",
      "    for subtree in tree.subtrees():\n",
      "        if subtree.node == 'CHUNK': print subtree\n",
      "\"\"\"\n",
      "for i in s:\n",
      "    print  cp.parse(s)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sentence = [(\"the\", \"DT\"), (\"little\", \"JJ\"), (\"yellow\", \"JJ\"),(\"dog\", \"NN\"), (\"barked\", \"VBD\"), (\"at\", \"IN\"),  (\"the\", \"DT\"), (\"cat\", \"NN\")]\n",
      "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
      "cp = nltk.RegexpParser(grammar)\n",
      "result = cp.parse(sentence)\n",
      "print result\n",
      "result.draw()\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import conll2000\n",
      "for i in range(150,220):\n",
      "    print \"---------\"+str(i)+\"--------------\"\n",
      "    print conll2000.chunked_sents('train.txt', chunk_types=['NP'])[i]\n",
      "    print conll2000.chunked_sents('train.txt', chunk_types=['NP'])[i][2]\n",
      "\n",
      "\"\"\"\n",
      "Observed sequences are:\n",
      "1) Most commmon patterns are DT and NN and NNS\n",
      "2) Adjective(s) followed by Noun (NN or NNS)\n",
      "3) Determinant DT followed by Adjective followed by NN or NNS\n",
      "4) Determinant DT followed by Noun (NN or NNS or both)\n",
      "\"\"\"\n",
      "grammar = \"\"\"NP: {<DT>?<JJ>*<NN>}\n",
      "                {<DT>?<NN|NNS>}\n",
      "                {<DT>?<JJ><NN|NNS>+}\"\"\"\n",
      "cp = nltk.RegexpParser(grammar)\n",
      "sentences = [\"The little yellow dog barked at the cat\",\n",
      "             \"He accepted the position of an assistant director.\",\n",
      "             \"The camera man did a good job at direction\",\n",
      "             \"Telephone is one of the greatest inventions of mankind\",\n",
      "             \"Big data boo is here to stay\",\n",
      "             \"Some of the classic movies have an offbeat story\",\n",
      "             \"The punishment assigned to a defendant found guilty by a court, or fixed by law for a particular offense.\",\n",
      "             \"Jane Austen was an English novelist whose works of romantic fiction, set among the landed gentry, earned her a place as one of the most widely read writers in English literature\",\n",
      "             \"The second half of the 20th century saw a proliferation of Austen scholarship and the emergence of a Janeite fan culture.\",\n",
      "             \"A familiar and effective place for the short sentence is at the end of a long paragraph\"]\n",
      "for sent in sentences:\n",
      "    print sent\n",
      "    print \"-----------\"\n",
      "    result = cp.parse(nltk.pos_tag(nltk.word_tokenize(sent)))\n",
      "    print result\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The little yellow dog barked at the cat\n",
        "-----------\n",
        "(S\n",
        "  (NP The/DT little/JJ yellow/NN)\n",
        "  (NP dog/NN)\n",
        "  barked/VBD\n",
        "  at/IN\n",
        "  (NP the/DT cat/NN))\n",
        "He accepted the position of an assistant director.\n",
        "-----------\n",
        "(S\n",
        "  He/PRP\n",
        "  accepted/VBD\n",
        "  (NP the/DT position/NN)\n",
        "  of/IN\n",
        "  (NP an/DT assistant/JJ director/NN)\n",
        "  ./.)\n",
        "The camera man did a good job at direction\n",
        "-----------\n",
        "(S\n",
        "  (NP The/DT camera/NN)\n",
        "  (NP man/NN)\n",
        "  did/VBD\n",
        "  (NP a/DT good/JJ job/NN)\n",
        "  at/IN\n",
        "  (NP direction/NN))"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Telephone is one of the greatest inventions of mankind\n",
        "-----------\n",
        "(S\n",
        "  Telephone/NNP\n",
        "  is/VBZ\n",
        "  one/CD\n",
        "  of/IN\n",
        "  the/DT\n",
        "  greatest/JJS\n",
        "  (NP inventions/NNS)\n",
        "  of/IN\n",
        "  (NP mankind/NN))\n",
        "Big data boo is here to stay\n",
        "-----------\n",
        "(S Big/NNP (NP data/NNS) boo/VBP is/VBZ here/RB to/TO stay/VB)\n",
        "Some of the classic movies have an offbeat story\n",
        "-----------\n",
        "(S\n",
        "  Some/DT\n",
        "  of/IN\n",
        "  the/DT\n",
        "  classic/JJ\n",
        "  (NP movies/NNS)\n",
        "  have/VBP\n",
        "  (NP an/DT offbeat/NN)\n",
        "  (NP story/NN))"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "The punishment assigned to a defendant found guilty by a court, or fixed by law for a particular offense.\n",
        "-----------\n",
        "(S\n",
        "  (NP The/DT punishment/NN)\n",
        "  assigned/VBD\n",
        "  to/TO\n",
        "  (NP a/DT defendant/NN)\n",
        "  found/VBD\n",
        "  (NP guilty/NN)\n",
        "  by/IN\n",
        "  (NP a/DT court/NN)\n",
        "  ,/,\n",
        "  or/CC\n",
        "  fixed/VBN\n",
        "  by/IN\n",
        "  (NP law/NN)\n",
        "  for/IN\n",
        "  (NP a/DT particular/JJ offense/NN)\n",
        "  ./.)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Jane Austen was an English novelist whose works of romantic fiction, set among the landed gentry, earned her a place as one of the most widely read writers in English literature\n",
        "-----------\n",
        "(S\n",
        "  Jane/NNP\n",
        "  Austen/NNP\n",
        "  was/VBD\n",
        "  (NP an/DT English/JJ novelist/NN)\n",
        "  whose/WP$\n",
        "  (NP works/NNS)\n",
        "  of/IN\n",
        "  (NP romantic/JJ fiction/NN)\n",
        "  ,/,\n",
        "  set/VBN\n",
        "  among/IN\n",
        "  (NP the/DT landed/JJ gentry/NN)\n",
        "  ,/,\n",
        "  earned/VBD\n",
        "  her/PRP\n",
        "  (NP a/DT place/NN)\n",
        "  as/IN\n",
        "  one/CD\n",
        "  of/IN\n",
        "  the/DT\n",
        "  most/RBS\n",
        "  widely/RB\n",
        "  read/JJ\n",
        "  (NP writers/NNS)\n",
        "  in/IN\n",
        "  English/NNP\n",
        "  (NP literature/NN))"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "The second half of the 20th century saw a proliferation of Austen scholarship and the emergence of a Janeite fan culture.\n",
        "-----------\n",
        "(S\n",
        "  The/DT\n",
        "  second/JJ\n",
        "  half/DT\n",
        "  of/IN\n",
        "  (NP the/DT 20th/JJ century/NN)\n",
        "  saw/VBD\n",
        "  (NP a/DT proliferation/NN)\n",
        "  of/IN\n",
        "  Austen/NNP\n",
        "  (NP scholarship/NN)\n",
        "  and/CC\n",
        "  (NP the/DT emergence/NN)\n",
        "  of/IN\n",
        "  a/DT\n",
        "  Janeite/NNP\n",
        "  (NP fan/NN)\n",
        "  (NP culture/NN)\n",
        "  ./.)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "A familiar and effective place for the short sentence is at the end of a long paragraph\n",
        "-----------\n",
        "(S\n",
        "  A/DT\n",
        "  familiar/JJ\n",
        "  and/CC\n",
        "  (NP effective/JJ place/NN)\n",
        "  for/IN\n",
        "  (NP the/DT short/JJ sentence/NN)\n",
        "  is/VBZ\n",
        "  at/IN\n",
        "  (NP the/DT end/NN)\n",
        "  of/IN\n",
        "  (NP a/DT long/JJ paragraph/NN))"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 146
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import conll2000\n",
      "cp = nltk.RegexpParser(\"\")\n",
      "test_sents = conll2000.chunked_sents('test.txt', chunk_types=['NP'])\n",
      "print cp.evaluate(test_sents)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "ChunkParse score:\n",
        "    IOB Accuracy:  43.4%\n",
        "    Precision:      0.0%\n",
        "    Recall:         0.0%\n",
        "    F-Measure:      0.0%\n"
       ]
      }
     ],
     "prompt_number": 116
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "grammar = r\"NP: {<[CDJNP].*>+}\"\n",
      "cp = nltk.RegexpParser(grammar)\n",
      "print cp.evaluate(test_sents)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "ChunkParse score:\n",
        "    IOB Accuracy:  87.7%\n",
        "    Precision:     70.6%\n",
        "    Recall:        67.8%\n",
        "    F-Measure:     69.2%\n"
       ]
      }
     ],
     "prompt_number": 117
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "nltk.help.upenn_tagset('NNS')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "help(nltk.chat.Chat)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Help on class Chat in module nltk.chat.util:\n",
        "\n",
        "class Chat(__builtin__.object)\n",
        " |  Methods defined here:\n",
        " |  \n",
        " |  __init__(self, pairs, reflections={})\n",
        " |      Initialize the chatbot.  Pairs is a list of patterns and responses.  Each\n",
        " |      pattern is a regular expression matching the user's statement or question,\n",
        " |      e.g. r'I like (.*)'.  For each such pattern a list of possible responses\n",
        " |      is given, e.g. ['Why do you like %1', 'Did you ever dislike %1'].  Material\n",
        " |      which is matched by parenthesized sections of the patterns (e.g. .*) is mapped to\n",
        " |      the numbered positions in the responses, e.g. %1.\n",
        " |      \n",
        " |      :type pairs: list of tuple\n",
        " |      :param pairs: The patterns and responses\n",
        " |      :type reflections: dict\n",
        " |      :param reflections: A mapping between first and second person expressions\n",
        " |      :rtype: None\n",
        " |  \n",
        " |  converse(self, quit='quit')\n",
        " |      # Hold a conversation with a chatbot\n",
        " |  \n",
        " |  respond(self, str)\n",
        " |      Generate a response to the user input.\n",
        " |      \n",
        " |      :type str: str\n",
        " |      :param str: The string to be mapped\n",
        " |      :rtype: str\n",
        " |  \n",
        " |  ----------------------------------------------------------------------\n",
        " |  Data descriptors defined here:\n",
        " |  \n",
        " |  __dict__\n",
        " |      dictionary for instance variables (if defined)\n",
        " |  \n",
        " |  __weakref__\n",
        " |      list of weak references to the object (if defined)\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "sent = \"who is Alfred Einstein and what does he do, does he live in US\"\n",
      "text = nltk.pos_tag(sent.split())\n",
      "out = nltk.ne_chunk(text)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "out"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 27,
       "text": [
        "Tree('S', [('who', 'WP'), ('is', 'VBZ'), Tree('PERSON', [('Alfred', 'NNP'), ('Einstein', 'NNP')]), ('and', 'CC'), ('what', 'WP'), ('does', 'VBZ'), ('he', 'PRP'), ('do,', '-NONE-'), ('does', 'VBZ'), ('he', 'PRP'), ('live', 'VB'), ('in', 'IN'), Tree('GSP', [('US', 'NNP')])])"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "orglist = []\n",
      "personlist = []\n",
      "gpelist = []\n",
      "\n",
      "for chunk in out:\n",
      "    if hasattr(chunk,'node'):\n",
      "        if chunk.node =='ORGANIZATION':\n",
      "            organization = ' '.join([c[0] for c in chunk.leaves()])\n",
      "            orglist.append(organization)\n",
      "        if chunk.node =='PERSON':\n",
      "            person = ' '.join([c[0] for c in chunk.leaves()])\n",
      "            personlist.append(person)\n",
      "        if chunk.node =='GPE' or chunk.node =='GSP' :\n",
      "            gpe = ' '.join([c[0] for c in chunk.leaves()])\n",
      "            gpelist.append(gpe)\n",
      "print orglist\n",
      "print personlist\n",
      "print gpelist\n",
      "            "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[]\n",
        "['Alfred Einstein']\n",
        "['US']\n"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "help(nltk.ne_chunk)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Help on function ne_chunk in module nltk.chunk:\n",
        "\n",
        "ne_chunk(tagged_tokens, binary=False)\n",
        "    Use NLTK's currently recommended named entity chunker to\n",
        "    chunk the given list of tagged tokens.\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "query = \"This is my house\"\n",
      "for i in query.split():\n",
      "    print i\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "This\n",
        "is\n",
        "my\n",
        "house\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s = \"This is a great {}\".format(\"place in Berkeley\")\n",
      "s"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "'This is a great place in Berkeley'"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ids = [str(i) for i in range(6)]\n",
      "ids\n",
      "s = \",\".join(ids)\n",
      "s"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "'0,1,2,3,4,5'"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s = [(1),(2),(1),(2),(3),(4)]\n",
      "st = list(set(s))\n",
      "st\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "[1, 2, 3, 4]"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "answer = \"A quick brown fox jumps over a lazy dog\"\n",
      "ans = answer.split()\n",
      "for i in range(len(ans)):\n",
      "    for j in range(i+1, len(ans)+1):\n",
      "        phrase = \" \".join(ans[i:j])\n",
      "        if len(phrase.split()) < 5:\n",
      "            print phrase"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "A\n",
        "A quick\n",
        "A quick brown\n",
        "A quick brown fox\n",
        "quick\n",
        "quick brown\n",
        "quick brown fox\n",
        "quick brown fox jumps\n",
        "brown\n",
        "brown fox\n",
        "brown fox jumps\n",
        "brown fox jumps over\n",
        "fox\n",
        "fox jumps\n",
        "fox jumps over\n",
        "fox jumps over a\n",
        "jumps\n",
        "jumps over\n",
        "jumps over a\n",
        "jumps over a lazy\n",
        "over\n",
        "over a\n",
        "over a lazy\n",
        "over a lazy dog\n",
        "a\n",
        "a lazy\n",
        "a lazy dog\n",
        "lazy\n",
        "lazy dog\n",
        "dog\n"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from collections import Counter"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "d = Counter(['this is a very','this is a very','this','are you me','my','myself this is'])\n",
      "print d.most_common(3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('this is a very', 2), ('this', 1), ('my', 1)]\n"
       ]
      }
     ],
     "prompt_number": 55
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s = \"Sonali\"\n",
      "s.lower()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 47,
       "text": [
        "'sonali'"
       ]
      }
     ],
     "prompt_number": 47
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}